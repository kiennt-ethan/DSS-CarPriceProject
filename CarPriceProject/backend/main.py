import uvicorn
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator
import pandas as pd
import numpy as np
import joblib
from catboost import CatBoostRegressor, Pool
import os
import traceback
import logging
import io
import shap
import json
from datetime import datetime
from collections import Counter

# --- TH∆Ø VI·ªÜN AI & CHATBOT ---
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import initialize_agent, AgentType, Tool
from langchain_community.tools import DuckDuckGoSearchRun

# --- 1. C·∫§U H√åNH C∆† B·∫¢N ---
# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
GEMINI_KEY = os.getenv("GEMINI_KEY")

# C·∫•u h√¨nh log
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AutoPrestige_Backend")

app = FastAPI(
    title="AutoPrestige AI System",
    description="H·ªá th·ªëng ƒë·ªãnh gi√° xe & Tr·ª£ l√Ω ·∫£o AI Real-time",
    version="3.0"
)

# C·∫•u h√¨nh CORS ƒë·ªÉ Frontend (React) g·ªçi ƒë∆∞·ª£c API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Trong m√¥i tr∆∞·ªùng Dev cho ph√©p t·∫•t c·∫£
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# --- C·∫§U H√åNH FILE L·ªäCH S·ª¨ ---
HISTORY_FILE = "history_db.json"
# H√†m ti·ªán √≠ch: Load l·ªãch s·ª≠
def load_history():
    if not os.path.exists(HISTORY_FILE):
        return []
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []
# H√†m ti·ªán √≠ch: L∆∞u l·ªãch s·ª≠
def save_history(record):
    history = load_history()
    # Th√™m timestamp
    record['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    record['id'] = len(history) + 1
    history.insert(0, record) # M·ªõi nh·∫•t l√™n ƒë·∫ßu
    
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)
# --- 2. KH·ªûI T·∫†O AI AGENT (GEMINI + INTERNET) ---
ai_agent_executor = None

if GEMINI_KEY:
    try:
        # 1. Kh·ªüi t·∫°o LLM (Google Gemini Pro)
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=GEMINI_KEY,
            temperature=0.3, # ƒê·ªô s√°ng t·∫°o th·∫•p ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c
            convert_system_message_to_human=True
        )
        
        # 2. Kh·ªüi t·∫°o C√¥ng c·ª• Search (Internet Access - Mi·ªÖn ph√≠)
        search = DuckDuckGoSearchRun()
        
        tools = [
            Tool(
                name="Internet Search",
                func=search.run,
                description="D√πng khi c·∫ßn tra c·ª©u gi√° xe hi·ªán t·∫°i, tin t·ª©c th·ªã tr∆∞·ªùng ho·∫∑c so s√°nh gi√° th·ª±c t·∫ø. ƒê·∫ßu v√†o l√† c√¢u h·ªèi t√¨m ki·∫øm."
            )
        ]
        
        # 3. T·∫°o Agent (Ng∆∞·ªùi ƒë·∫°i di·ªán)
        ai_agent_executor = initialize_agent(
            tools, 
            llm, 
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
            verbose=True,
            handle_parsing_errors=True
        )
        logger.info("‚úÖ AI Agent (Gemini + Internet) ƒë√£ s·∫µn s√†ng!")
        
    except Exception as e:
        logger.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o AI Agent: {e}")
else:
    logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GEMINI_KEY trong .env. Ch·ª©c nƒÉng Chatbot s·∫Ω b·ªã t·∫Øt.")


# --- 3. LOAD MODEL CATBOOST & RESOURCES ---
MODEL_PATH = "catboost_final_model.cbm"
ENCODER_PATH = "encoders.pkl"

model = CatBoostRegressor()
encoders = {}
explainer = None

try:
    # Load Model CatBoost
    if os.path.exists(MODEL_PATH):
        model.load_model(MODEL_PATH)
        logger.info(f"‚úÖ CatBoost Model loaded: {MODEL_PATH}")
        # Kh·ªüi t·∫°o SHAP Explainer (ƒë·ªÉ gi·∫£i th√≠ch t·∫°i sao ra gi√° ƒë√≥)
        try:
            explainer = shap.TreeExplainer(model)
            logger.info("‚úÖ SHAP Explainer initialized.")
        except Exception as shap_err:
            logger.warning(f"‚ö†Ô∏è SHAP Error: {shap_err}")
    else:
        logger.critical(f"‚ùå Model missing: {MODEL_PATH}")

    # Load Encoders (N·∫øu c√≥ d√πng LabelEncoding l√∫c train)
    if os.path.exists(ENCODER_PATH):
        encoders = joblib.load(ENCODER_PATH)
        logger.info(f"‚úÖ Encoders loaded ({len(encoders)} encoders).")
except Exception as e:
    logger.error(f"‚ùå Resource Loading Error: {e}")

# --- 4. DATA MODELS (INPUT SCHEMAS) ---
class CarInput(BaseModel):
    manufacturer: str
    model: str
    year: int = Field(..., ge=1970, le=2026)
    transmission: str
    mileage: int = Field(..., ge=0)
    fuelType: str
    tax: float = Field(..., ge=0)
    mpg: float = Field(..., ge=0)
    engineSize: float = Field(..., ge=0)

    @validator('manufacturer', 'model', 'transmission', 'fuelType')
    def clean_strings(cls, v): 
        return str(v).strip()

class ChatInput(BaseModel):
    message: str

# --- 5. H√ÄM X·ª¨ L√ù TRUNG T√ÇM (CORE LOGIC) ---
def preprocess_and_predict(input_df: pd.DataFrame, explain: bool = False):
    """
    H√†m x·ª≠ l√Ω chung cho c·∫£ Single Predict v√† Batch Predict.
    """
    try:
        # A. Feature Engineering (T·∫°o bi·∫øn CarAge n·∫øu ch∆∞a c√≥)
        if 'CarAge' not in input_df.columns and 'year' in input_df.columns:
            input_df['CarAge'] = 2025 - input_df['year']
        
        # B. Encoding (X·ª≠ l√Ω bi·∫øn ph√¢n lo·∫°i)
        # Map t√™n c·ªôt t·ª´ Frontend -> T√™n c·ªôt trong Encoders/Model
        # V√≠ d·ª•: frontend g·ª≠i 'manufacturer', model c·∫ßn 'Manufacturer' ho·∫∑c 'Manufacturer_Code'
        col_mapping = {
            'manufacturer': 'Manufacturer', 
            'Manufacturer': 'Manufacturer', 
            'model': 'model'
        }
        
        for input_col, encoder_key in col_mapping.items():
            if input_col in input_df.columns and encoder_key in encoders:
                encoder = encoders[encoder_key]
                
                # H√†m an to√†n: N·∫øu g·∫∑p gi√° tr·ªã l·∫° ch∆∞a t·ª´ng th·∫•y -> g√°n 0
                def safe_transform(val):
                    val = str(val)
                    if val in encoder.classes_:
                        return encoder.transform([val])[0]
                    else:
                        return 0 
                
                # T·∫°o c·ªôt _Code m·ªõi
                target_col = f"{encoder_key}_Code"
                input_df[target_col] = input_df[input_col].apply(safe_transform)

        # C. Chu·∫©n b·ªã DataFrame cu·ªëi c√πng (Final Alignment)
        # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt ƒë√∫ng y h·ªát l√∫c train model
        required_features = model.feature_names_
        final_df = pd.DataFrame(index=input_df.index)

        for feature in required_features:
            if feature in input_df.columns:
                final_df[feature] = input_df[feature]
            elif '_' in feature: # X·ª≠ l√Ω One-Hot Encoding ƒë·ªông
                prefix = feature.split('_')[0]
                value = "_".join(feature.split('_')[1:])
                found = False
                for col in input_df.columns:
                    # So s√°nh kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
                    if col.lower() == prefix.lower():
                        final_df[feature] = (input_df[col].astype(str) == value).astype(int)
                        found = True
                        break
                if not found: 
                    final_df[feature] = 0
            else:
                # N·∫øu thi·∫øu c·ªôt s·ªë -> ƒêi·ªÅn 0
                final_df[feature] = 0 

        # D. Predict
        preds = model.predict(final_df)
        preds = np.maximum(preds, 0) # Gi√° xe kh√¥ng ƒë∆∞·ª£c √¢m

        # E. Gi·∫£i th√≠ch (SHAP)
        shap_explanation = None
        if explain and explainer and len(final_df) == 1:
            try:
                shap_values = explainer.shap_values(final_df)
                vals = shap_values[0]
                # L·∫•y Top 3 ƒë·∫∑c tr∆∞ng ·∫£nh h∆∞·ªüng nh·∫•t
                feature_importance = pd.DataFrame({
                    'feature': required_features,
                    'shap': vals,
                    'abs_shap': np.abs(vals)
                }).sort_values('abs_shap', ascending=False).head(3)
                
                shap_explanation = feature_importance[['feature', 'shap']].to_dict(orient='records')
            except Exception as e:
                logger.warning(f"SHAP calculation failed: {e}")

        return preds, shap_explanation

    except Exception as e:
        logger.error(f"Preprocessing failed: {e}")
        traceback.print_exc()
        raise e

# --- 6. API ENDPOINTS ---

@app.get("/")
def home():
    return {"status": "AutoPrestige Backend is Running üöÄ"}

# API 1: ƒê·ªãnh gi√° ƒë∆°n l·∫ª
@app.post("/predict")
def predict_single(data: CarInput):
    try:
        df_input = pd.DataFrame([data.dict()])
        # 1. D·ª± ƒëo√°n
        price_pred, explanation = preprocess_and_predict(df_input, explain=True)
        final_price = round(price_pred[0], 2)

        # 2. L∆ØU V√ÄO L·ªäCH S·ª¨ (NEW)
        history_record = data.dict()
        history_record['predicted_price'] = final_price
        history_record['explanation'] = explanation
        save_history(history_record) # <--- L∆∞u t·∫°i ƒë√¢y
        
        return {
            "price": final_price,
            "currency": "USD",
            "explanation": explanation
        }
    except Exception as e:
        logger.error(f"Prediction Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/dashboard-stats")
def get_dashboard_stats():
    history = load_history()
    if not history:
        return {"stats": [], "chart_data": [], "brands": [], "recent": []}

    total_predictions = len(history)
    total_value = sum(item['predicted_price'] for item in history)
    
    # 1. T√≠nh to√°n th·ªëng k√™ theo H√£ng (Cho bi·ªÉu ƒë·ªì tr√≤n/c·ªôt)
    brands = [item['manufacturer'] for item in history]
    brand_counts = Counter(brands)
    brand_data = [
        {"name": k, "value": v, "color": "#" + "".join([hex(np.random.randint(0,255))[2:].zfill(2) for _ in range(3)])} 
        for k, v in brand_counts.most_common(5)
    ]

    # 2. D·ªØ li·ªáu bi·ªÉu ƒë·ªì gi√° (Gi·∫£ l·∫≠p theo th·ªùi gian th·ª±c t·∫ø ho·∫∑c index)
    # L·∫•y 10 giao d·ªãch g·∫ßn nh·∫•t ƒë·∫£o ng∆∞·ª£c l·∫°i ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
    recent_10 = history[:10][::-1] 
    chart_data = [{"name": f"#{item['id']}", "price": item['predicted_price']} for item in recent_10]

    return {
        "stats": [
            {"label": "T·ªïng l∆∞·ª£t ƒë·ªãnh gi√°", "value": str(total_predictions), "change": "+Realtime", "isPos": True, "icon": "Car", "color": "blue"},
            {"label": "T·ªïng gi√° tr·ªã", "value": f"${total_value:,.0f}", "change": "Market Cap", "isPos": True, "icon": "DollarSign", "color": "green"},
            {"label": "H√£ng ph·ªï bi·∫øn nh·∫•t", "value": brand_counts.most_common(1)[0][0] if brands else "N/A", "change": "Top 1", "isPos": True, "icon": "Users", "color": "purple"},
        ],
        "brand_data": brand_data,
        "chart_data": chart_data,
        "recent": history[:5] # 5 giao d·ªãch m·ªõi nh·∫•t
    }

@app.get("/history")
def get_full_history():
    return load_history()

# API 2: ƒê·ªãnh gi√° h√†ng lo·∫°t (Excel/CSV)
@app.post("/predict-batch")
async def predict_batch(file: UploadFile = File(...)):
    try:
        contents = await file.read()
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(contents))
        elif file.filename.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(io.BytesIO(contents))
        else:
            raise HTTPException(400, "Ch·ªâ h·ªó tr·ª£ file .csv ho·∫∑c .xlsx")
            
        if df.empty:
            raise HTTPException(400, "File r·ªóng")
        
        # -----------------------------------------------------------
        # -- ƒê√ÇY L√Ä D√íNG CODE S·ª¨A L·ªñI QUAN TR·ªåNG --
        # Chu·∫©n h√≥a t√™n c·ªôt v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ ƒë·ªìng nh·∫•t v·ªõi Pydantic model
        df.columns = [col.lower() for col in df.columns]
        # -----------------------------------------------------------

        # 1. D·ª± ƒëo√°n
        predictions, _ = preprocess_and_predict(df, explain=False)
        
        # 2. G√°n k·∫øt qu·∫£
        df['predicted_price'] = np.round(predictions, 2)
        
        # 3. L∆ØU V√ÄO L·ªäCH S·ª¨ (BATCH SAVE)
        records = df.to_dict(orient="records")
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        history = load_history()
        start_id = len(history) + 1
        
        new_history_items = []
        for i, item in enumerate(records):
            clean_item = {k: (v if pd.notna(v) else "") for k, v in item.items()}
            clean_item['id'] = start_id + i
            clean_item['timestamp'] = current_time
            clean_item['explanation'] = None
            new_history_items.append(clean_item)
            
        full_history = new_history_items + history
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(full_history, f, ensure_ascii=False, indent=4)

        return {"data": df.fillna("").to_dict(orient="records")}
    
    except Exception as e:
        logger.error(f"Batch Error: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}")

# API 3: AI Chatbot (Gemini + Internet)
@app.post("/chat")
async def chat_agent(data: ChatInput):
    """
    Chatbot th√¥ng minh:
    - Nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng.
    - AI t·ª± quy·∫øt ƒë·ªãnh tr·∫£ l·ªùi b·∫±ng ki·∫øn th·ª©c c√≥ s·∫µn ho·∫∑c tra Google (DuckDuckGo).
    """
    if not ai_agent_executor:
        return {"response": "H·ªá th·ªëng AI ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh (Thi·∫øu API Key). Vui l√≤ng ki·ªÉm tra server."}
    
    try:
        # Prompt Engineering: ƒê·ªãnh h∆∞·ªõng cho Gemini
        system_instruction = (
            "B·∫°n l√† chuy√™n gia ƒë·ªãnh gi√° xe h∆°i c·ªßa h·ªá th·ªëng AutoPrestige. "
            "H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát. "
            "N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ gi√° th·ªã tr∆∞·ªùng hi·ªán t·∫°i, h√£y D√ôNG C√îNG C·ª§ 'Internet Search' ƒë·ªÉ t√¨m th√¥ng tin m·ªõi nh·∫•t. "
            "Kh√¥ng ƒë∆∞·ª£c t·ª± b·ªãa ra gi√° n·∫øu kh√¥ng t√¨m th·∫•y. "
            f"C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {data.message}"
        )
        
        # Ch·∫°y Agent
        response = ai_agent_executor.run(system_instruction)
        return {"response": response}
    
    except Exception as e:
        logger.error(f"Chat Error: {e}")
        return {"response": f"Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë khi suy nghƒ©: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)